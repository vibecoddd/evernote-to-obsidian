#!/usr/bin/env python3
"""
å»é‡ç®¡ç†å™¨ - é˜²æ­¢å¤šæ¬¡å¯¼å‡ºæ—¶äº§ç”Ÿé‡å¤ç¬”è®°
"""

import os
import json
import hashlib
from pathlib import Path
from typing import Dict, Set, List, Optional, Tuple
from datetime import datetime

class DeduplicationManager:
    """å»é‡ç®¡ç†å™¨"""

    def __init__(self, output_dir: str):
        self.output_dir = Path(output_dir)
        self.dedup_db_path = self.output_dir / '.migration_history.json'
        self.history = self._load_history()

    def _load_history(self) -> Dict:
        """åŠ è½½å†å²è®°å½•"""
        if self.dedup_db_path.exists():
            try:
                with open(self.dedup_db_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"âš ï¸ åŠ è½½å†å²è®°å½•å¤±è´¥: {e}")

        return {
            'migrations': [],
            'processed_notes': {},  # note_id -> file_info
            'processed_files': {},  # file_hash -> file_path
            'deleted_notes': {},    # note_id -> deletion_info
            'last_migration': None
        }

    def _save_history(self):
        """ä¿å­˜å†å²è®°å½•"""
        try:
            self.output_dir.mkdir(parents=True, exist_ok=True)
            with open(self.dedup_db_path, 'w', encoding='utf-8') as f:
                json.dump(self.history, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"âŒ ä¿å­˜å†å²è®°å½•å¤±è´¥: {e}")

    def start_migration(self, migration_id: str, source_info: Dict) -> Dict:
        """å¼€å§‹æ–°çš„è¿ç§»ä¼šè¯"""
        migration_info = {
            'migration_id': migration_id,
            'start_time': datetime.now().isoformat(),
            'source_info': source_info,
            'status': 'running',
            'stats': {
                'total_notes': 0,
                'new_notes': 0,
                'skipped_duplicates': 0,
                'updated_notes': 0
            }
        }

        self.history['migrations'].append(migration_info)
        self.history['last_migration'] = migration_id
        self._save_history()

        print(f"ğŸ†• å¼€å§‹è¿ç§»ä¼šè¯: {migration_id}")

        # æ£€æŸ¥æ˜¯å¦æœ‰ä¹‹å‰çš„è¿ç§»
        previous_migrations = [m for m in self.history['migrations'] if m['migration_id'] != migration_id]
        if previous_migrations:
            print(f"ğŸ“‹ å‘ç° {len(previous_migrations)} ä¸ªå†å²è¿ç§»")
            print(f"ğŸ“Š å·²å¤„ç†ç¬”è®°: {len(self.history['processed_notes'])} ä¸ª")

        return migration_info

    def is_note_deleted(self, note_id: str) -> bool:
        """æ£€æŸ¥ç¬”è®°æ˜¯å¦å·²åˆ é™¤"""
        return note_id in self.history['deleted_notes']

    def mark_note_deleted(self, note_id: str, note_title: str = "Unknown"):
        """æ ‡è®°ç¬”è®°ä¸ºå·²åˆ é™¤"""
        deletion_info = {
            'deleted_time': datetime.now().isoformat(),
            'title': note_title,
            'migration_id': self.history['last_migration']
        }

        self.history['deleted_notes'][note_id] = deletion_info

        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œåˆ é™¤å®ƒ
        if note_id in self.history['processed_notes']:
            note_info = self.history['processed_notes'][note_id]
            output_file = note_info.get('output_file')

            if output_file and Path(output_file).exists():
                try:
                    Path(output_file).unlink()
                    print(f"ğŸ—‘ï¸ å·²åˆ é™¤ç¬”è®°æ–‡ä»¶: {Path(output_file).name}")
                except Exception as e:
                    print(f"âŒ åˆ é™¤æ–‡ä»¶å¤±è´¥: {e}")

            # ä»å·²å¤„ç†è®°å½•ä¸­ç§»é™¤
            del self.history['processed_notes'][note_id]

        print(f"ğŸ—‘ï¸ æ ‡è®°ç¬”è®°å·²åˆ é™¤: {note_title}")

    def detect_deleted_notes(self, current_note_ids: Set[str]):
        """
        æ£€æµ‹å·²åˆ é™¤çš„ç¬”è®°

        Args:
            current_note_ids: å½“å‰å¯¼å‡ºä¸­çš„ç¬”è®°IDé›†åˆ
        """
        previously_processed = set(self.history['processed_notes'].keys())
        deleted_note_ids = previously_processed - current_note_ids

        if deleted_note_ids:
            print(f"ğŸ” æ£€æµ‹åˆ° {len(deleted_note_ids)} ä¸ªå·²åˆ é™¤çš„ç¬”è®°")

            for note_id in deleted_note_ids:
                if note_id not in self.history['deleted_notes']:
                    note_info = self.history['processed_notes'].get(note_id, {})
                    title = note_info.get('title', 'Unknown')
                    self.mark_note_deleted(note_id, title)

    def should_process_note(self, note_data: Dict) -> Tuple[bool, str]:
        """
        æ£€æŸ¥æ˜¯å¦åº”è¯¥å¤„ç†è¿™ä¸ªç¬”è®°
        è¿”å›: (should_process, reason)
        """
        note_id = note_data.get('guid') or note_data.get('id')
        title = note_data.get('title', 'Untitled')
        content = note_data.get('content', '')

        # 0. æ£€æŸ¥ç¬”è®°æ˜¯å¦å·²è¢«æ ‡è®°ä¸ºåˆ é™¤
        if note_id and self.is_note_deleted(note_id):
            # å¦‚æœç¬”è®°é‡æ–°å‡ºç°ï¼Œä»åˆ é™¤åˆ—è¡¨ä¸­ç§»é™¤
            del self.history['deleted_notes'][note_id]
            print(f"ğŸ”„ ç¬”è®°é‡æ–°å‡ºç°ï¼Œä»åˆ é™¤åˆ—è¡¨ç§»é™¤: {title}")

        # 1. æ£€æŸ¥ç¬”è®°IDæ˜¯å¦å·²å­˜åœ¨
        if note_id and note_id in self.history['processed_notes']:
            existing_info = self.history['processed_notes'][note_id]

            # æ£€æŸ¥å†…å®¹æ˜¯å¦æœ‰æ›´æ–°
            content_hash = self._calculate_content_hash(content)
            if existing_info.get('content_hash') == content_hash:
                return False, f"ç¬”è®°å·²å­˜åœ¨ä¸”å†…å®¹æœªå˜åŒ–: {title}"
            else:
                return True, f"ç¬”è®°å†…å®¹å·²æ›´æ–°ï¼Œéœ€è¦é‡æ–°å¤„ç†: {title}"

        # 2. æ£€æŸ¥å†…å®¹å“ˆå¸Œ
        content_hash = self._calculate_content_hash(content)
        for processed_id, info in self.history['processed_notes'].items():
            if info.get('content_hash') == content_hash:
                return False, f"å‘ç°å†…å®¹é‡å¤ç¬”è®°: {title} (é‡å¤äº {info.get('title', 'Unknown')})"

        # 3. æ£€æŸ¥æ ‡é¢˜é‡å¤ï¼ˆå¯é€‰ï¼Œå¯èƒ½æœ‰åˆç†çš„é‡å¤æ ‡é¢˜ï¼‰
        title_normalized = self._normalize_title(title)
        similar_notes = []
        for processed_id, info in self.history['processed_notes'].items():
            if self._normalize_title(info.get('title', '')) == title_normalized:
                similar_notes.append(info.get('title'))

        if similar_notes:
            print(f"âš ï¸ å‘ç°ç›¸ä¼¼æ ‡é¢˜: {title} (ä¸ {len(similar_notes)} ä¸ªç¬”è®°æ ‡é¢˜ç›¸ä¼¼)")

        return True, f"æ–°ç¬”è®°ï¼Œå¯ä»¥å¤„ç†: {title}"

    def should_process_file(self, file_path: str, content: str = None) -> Tuple[bool, str]:
        """
        æ£€æŸ¥æ˜¯å¦åº”è¯¥å¤„ç†è¿™ä¸ªæ–‡ä»¶
        è¿”å›: (should_process, reason)
        """
        file_path = Path(file_path)

        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œæ£€æŸ¥å†…å®¹
        if file_path.exists():
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    existing_content = f.read()

                if content and existing_content == content:
                    return False, f"æ–‡ä»¶å†…å®¹ç›¸åŒï¼Œè·³è¿‡: {file_path.name}"
                elif content:
                    return True, f"æ–‡ä»¶å†…å®¹ä¸åŒï¼Œéœ€è¦æ›´æ–°: {file_path.name}"
                else:
                    return False, f"æ–‡ä»¶å·²å­˜åœ¨: {file_path.name}"
            except Exception:
                return True, f"æ— æ³•è¯»å–ç°æœ‰æ–‡ä»¶ï¼Œé‡æ–°åˆ›å»º: {file_path.name}"

        return True, f"æ–°æ–‡ä»¶: {file_path.name}"

    def mark_note_processed(self, note_data: Dict, output_file: str, is_update: bool = False):
        """æ ‡è®°ç¬”è®°å·²å¤„ç†"""
        note_id = note_data.get('guid') or note_data.get('id')
        title = note_data.get('title', 'Untitled')
        content = note_data.get('content', '')

        if not note_id:
            # å¦‚æœæ²¡æœ‰IDï¼Œç”Ÿæˆä¸€ä¸ªåŸºäºå†…å®¹çš„ID
            note_id = self._calculate_content_hash(content)

        note_info = {
            'note_id': note_id,
            'title': title,
            'content_hash': self._calculate_content_hash(content),
            'output_file': str(output_file),
            'processed_time': datetime.now().isoformat(),
            'migration_id': self.history['last_migration'],
            'is_update': is_update,
            'file_size': Path(output_file).stat().st_size if Path(output_file).exists() else 0
        }

        self.history['processed_notes'][note_id] = note_info
        self._update_migration_stats('updated_notes' if is_update else 'new_notes')

        action = "æ›´æ–°" if is_update else "æ–°å»º"
        print(f"âœ… {action}ç¬”è®°: {title} -> {Path(output_file).name}")

    def mark_note_skipped(self, note_data: Dict, reason: str):
        """æ ‡è®°ç¬”è®°å·²è·³è¿‡"""
        title = note_data.get('title', 'Untitled')
        self._update_migration_stats('skipped_duplicates')
        print(f"â­ï¸ è·³è¿‡ç¬”è®°: {title} - {reason}")

    def mark_file_processed(self, file_path: str, content_hash: str = None):
        """æ ‡è®°æ–‡ä»¶å·²å¤„ç†"""
        file_path = Path(file_path)

        if not content_hash and file_path.exists():
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                content_hash = self._calculate_content_hash(content)
            except Exception:
                content_hash = str(file_path.stat().st_mtime)

        self.history['processed_files'][content_hash] = str(file_path)

    def finish_migration(self, success: bool = True, error: str = None):
        """å®Œæˆè¿ç§»ä¼šè¯"""
        migration_id = self.history['last_migration']
        if not migration_id:
            return

        # æ›´æ–°è¿ç§»ä¿¡æ¯
        for migration in self.history['migrations']:
            if migration['migration_id'] == migration_id:
                migration['status'] = 'completed' if success else 'failed'
                migration['end_time'] = datetime.now().isoformat()
                if error:
                    migration['error'] = error
                break

        self._save_history()

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        current_migration = next((m for m in self.history['migrations']
                               if m['migration_id'] == migration_id), None)

        if current_migration:
            stats = current_migration['stats']
            print(f"\nğŸ“Š è¿ç§»ç»Ÿè®¡ ({migration_id}):")
            print(f"   æ€»è®¡ç¬”è®°: {stats['total_notes']}")
            print(f"   æ–°å»ºç¬”è®°: {stats['new_notes']}")
            print(f"   æ›´æ–°ç¬”è®°: {stats['updated_notes']}")
            print(f"   è·³è¿‡é‡å¤: {stats['skipped_duplicates']}")

    def get_migration_summary(self) -> Dict:
        """è·å–è¿ç§»æ‘˜è¦"""
        total_migrations = len(self.history['migrations'])
        total_notes = len(self.history['processed_notes'])

        recent_migration = None
        if self.history['migrations']:
            recent_migration = self.history['migrations'][-1]

        return {
            'total_migrations': total_migrations,
            'total_processed_notes': total_notes,
            'recent_migration': recent_migration,
            'deduplication_enabled': True
        }

    def clean_orphaned_files(self, current_files: Set[str]) -> int:
        """æ¸…ç†å­¤ç«‹æ–‡ä»¶"""
        cleaned_count = 0

        # æ£€æŸ¥å†å²è®°å½•ä¸­çš„æ–‡ä»¶æ˜¯å¦ä»ç„¶å­˜åœ¨
        valid_notes = {}
        for note_id, note_info in self.history['processed_notes'].items():
            output_file = note_info.get('output_file')
            if output_file and Path(output_file).exists():
                valid_notes[note_id] = note_info
            else:
                cleaned_count += 1

        if cleaned_count > 0:
            self.history['processed_notes'] = valid_notes
            self._save_history()
            print(f"ğŸ§¹ æ¸…ç†äº† {cleaned_count} ä¸ªå­¤ç«‹çš„æ–‡ä»¶è®°å½•")

        return cleaned_count

    def _calculate_content_hash(self, content: str) -> str:
        """è®¡ç®—å†…å®¹å“ˆå¸Œ"""
        # æ ‡å‡†åŒ–å†…å®¹
        normalized = content.strip()
        # å»é™¤å¤šä½™ç©ºè¡Œ
        normalized = '\n'.join(line.rstrip() for line in normalized.split('\n'))
        return hashlib.md5(normalized.encode('utf-8')).hexdigest()

    def _normalize_title(self, title: str) -> str:
        """æ ‡å‡†åŒ–æ ‡é¢˜"""
        return title.strip().lower()

    def _update_migration_stats(self, stat_key: str):
        """æ›´æ–°è¿ç§»ç»Ÿè®¡"""
        migration_id = self.history['last_migration']
        if not migration_id:
            return

        for migration in self.history['migrations']:
            if migration['migration_id'] == migration_id:
                migration['stats'][stat_key] += 1
                migration['stats']['total_notes'] += 1
                break

def create_dedup_manager(config: Dict) -> DeduplicationManager:
    """åˆ›å»ºå»é‡ç®¡ç†å™¨"""
    output_dir = config.get('output', {}).get('obsidian_vault', '/tmp/obsidian_vault')
    return DeduplicationManager(output_dir)