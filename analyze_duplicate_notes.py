#!/usr/bin/env python3
"""
åˆ†æå’Œè§£å†³ç¬”è®°é‡å¤é—®é¢˜
"""

import os
import sys
from pathlib import Path
import hashlib
import json
from typing import List, Dict, Set, Tuple
from datetime import datetime

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
src_dir = Path(__file__).parent / 'src'
sys.path.insert(0, str(src_dir))

class DuplicateNotesAnalyzer:
    """ç¬”è®°é‡å¤åˆ†æå™¨"""

    def __init__(self, vault_path: str):
        self.vault_path = Path(vault_path)
        self.duplicate_groups = []
        self.stats = {
            'total_notes': 0,
            'duplicate_notes': 0,
            'duplicate_groups': 0,
            'space_saved_mb': 0
        }

    def find_duplicates(self) -> List[List[Path]]:
        """æŸ¥æ‰¾é‡å¤ç¬”è®°"""
        print("ğŸ” åˆ†æç¬”è®°é‡å¤é—®é¢˜...")

        if not self.vault_path.exists():
            print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {self.vault_path}")
            return []

        # æ”¶é›†æ‰€æœ‰markdownæ–‡ä»¶
        md_files = list(self.vault_path.rglob('*.md'))
        self.stats['total_notes'] = len(md_files)

        print(f"ğŸ“Š æ‰¾åˆ° {len(md_files)} ä¸ªç¬”è®°æ–‡ä»¶")

        if len(md_files) == 0:
            print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç¬”è®°æ–‡ä»¶")
            return []

        # åˆ†æé‡å¤ç±»å‹
        content_groups = self._group_by_content_hash(md_files)
        title_groups = self._group_by_title(md_files)
        filename_groups = self._group_by_filename(md_files)

        # åˆå¹¶é‡å¤ç»„
        all_groups = []
        all_groups.extend([group for group in content_groups if len(group) > 1])
        all_groups.extend([group for group in title_groups if len(group) > 1])
        all_groups.extend([group for group in filename_groups if len(group) > 1])

        # å»é‡åˆå¹¶çš„ç»„
        self.duplicate_groups = self._dedupe_groups(all_groups)

        self.stats['duplicate_groups'] = len(self.duplicate_groups)
        self.stats['duplicate_notes'] = sum(len(group) - 1 for group in self.duplicate_groups)

        return self.duplicate_groups

    def _group_by_content_hash(self, files: List[Path]) -> List[List[Path]]:
        """æŒ‰å†…å®¹å“ˆå¸Œåˆ†ç»„"""
        print("ğŸ“‹ æŒ‰å†…å®¹å“ˆå¸Œåˆ†ç»„...")

        content_hash_map = {}

        for file_path in files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                # æ ‡å‡†åŒ–å†…å®¹ï¼ˆå»é™¤ç©ºç™½å­—ç¬¦å·®å¼‚ï¼‰
                normalized_content = self._normalize_content(content)
                content_hash = hashlib.md5(normalized_content.encode()).hexdigest()

                if content_hash not in content_hash_map:
                    content_hash_map[content_hash] = []
                content_hash_map[content_hash].append(file_path)

            except Exception as e:
                print(f"âš ï¸ è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")

        return list(content_hash_map.values())

    def _group_by_title(self, files: List[Path]) -> List[List[Path]]:
        """æŒ‰æ ‡é¢˜åˆ†ç»„"""
        print("ğŸ“‹ æŒ‰ç¬”è®°æ ‡é¢˜åˆ†ç»„...")

        title_map = {}

        for file_path in files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                # æå–æ ‡é¢˜
                title = self._extract_title(content, file_path)
                normalized_title = title.strip().lower()

                if normalized_title not in title_map:
                    title_map[normalized_title] = []
                title_map[normalized_title].append(file_path)

            except Exception as e:
                print(f"âš ï¸ è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")

        return list(title_map.values())

    def _group_by_filename(self, files: List[Path]) -> List[List[Path]]:
        """æŒ‰æ–‡ä»¶ååˆ†ç»„"""
        print("ğŸ“‹ æŒ‰æ–‡ä»¶ååˆ†ç»„...")

        filename_map = {}

        for file_path in files:
            # å»é™¤æ‰©å±•åï¼Œæ ‡å‡†åŒ–æ–‡ä»¶å
            stem = file_path.stem.strip().lower()

            if stem not in filename_map:
                filename_map[stem] = []
            filename_map[stem].append(file_path)

        return list(filename_map.values())

    def _normalize_content(self, content: str) -> str:
        """æ ‡å‡†åŒ–å†…å®¹"""
        # å»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        lines = content.split('\n')
        normalized_lines = []

        for line in lines:
            # å»é™¤è¡Œé¦–è¡Œå°¾ç©ºæ ¼
            line = line.strip()
            # è·³è¿‡ç©ºè¡Œ
            if line:
                normalized_lines.append(line)

        return '\n'.join(normalized_lines)

    def _extract_title(self, content: str, file_path: Path) -> str:
        """æå–ç¬”è®°æ ‡é¢˜"""
        lines = content.split('\n')

        # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªéç©ºè¡Œä½œä¸ºæ ‡é¢˜
        for line in lines:
            line = line.strip()
            if line:
                # å¦‚æœæ˜¯markdownæ ‡é¢˜æ ¼å¼
                if line.startswith('#'):
                    return line.lstrip('#').strip()
                else:
                    return line

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡é¢˜ï¼Œä½¿ç”¨æ–‡ä»¶å
        return file_path.stem

    def _dedupe_groups(self, groups: List[List[Path]]) -> List[List[Path]]:
        """å»é‡é‡å¤ç»„"""
        seen_files = set()
        unique_groups = []

        for group in groups:
            if len(group) <= 1:
                continue

            # æ£€æŸ¥æ˜¯å¦å·²ç»åœ¨å…¶ä»–ç»„ä¸­å¤„ç†è¿‡
            group_files = set(group)
            if group_files & seen_files:
                continue

            unique_groups.append(group)
            seen_files.update(group_files)

        return unique_groups

    def generate_report(self) -> str:
        """ç”Ÿæˆé‡å¤åˆ†ææŠ¥å‘Š"""
        report = []
        report.append("ğŸ“‹ ç¬”è®°é‡å¤åˆ†ææŠ¥å‘Š")
        report.append("=" * 50)
        report.append(f"æ€»ç¬”è®°æ•°: {self.stats['total_notes']}")
        report.append(f"é‡å¤ç»„æ•°: {self.stats['duplicate_groups']}")
        report.append(f"é‡å¤ç¬”è®°æ•°: {self.stats['duplicate_notes']}")
        report.append("")

        if not self.duplicate_groups:
            report.append("âœ… æ²¡æœ‰å‘ç°é‡å¤ç¬”è®°")
            return '\n'.join(report)

        report.append("ğŸ” å‘ç°çš„é‡å¤ç»„:")
        report.append("")

        for i, group in enumerate(self.duplicate_groups, 1):
            report.append(f"é‡å¤ç»„ {i} ({len(group)} ä¸ªæ–‡ä»¶):")

            for j, file_path in enumerate(group):
                file_size = file_path.stat().st_size
                mod_time = datetime.fromtimestamp(file_path.stat().st_mtime)

                marker = "ğŸ“ [ä¿ç•™]" if j == 0 else "ğŸ—‘ï¸ [åˆ é™¤]"
                report.append(f"  {marker} {file_path.name}")
                report.append(f"     è·¯å¾„: {file_path}")
                report.append(f"     å¤§å°: {file_size} bytes")
                report.append(f"     ä¿®æ”¹æ—¶é—´: {mod_time}")

            report.append("")

        return '\n'.join(report)

    def create_deduplication_plan(self) -> Dict:
        """åˆ›å»ºå»é‡è®¡åˆ’"""
        plan = {
            'keep_files': [],
            'remove_files': [],
            'backup_info': {}
        }

        for group in self.duplicate_groups:
            if len(group) <= 1:
                continue

            # é€‰æ‹©ä¿ç•™ç­–ç•¥ï¼šä¿ç•™æœ€æ–°çš„æ–‡ä»¶
            group_sorted = sorted(group, key=lambda x: x.stat().st_mtime, reverse=True)

            keep_file = group_sorted[0]
            remove_files = group_sorted[1:]

            plan['keep_files'].append(str(keep_file))

            for remove_file in remove_files:
                plan['remove_files'].append(str(remove_file))
                plan['backup_info'][str(remove_file)] = {
                    'kept_as': str(keep_file),
                    'original_size': remove_file.stat().st_size,
                    'original_mtime': remove_file.stat().st_mtime
                }

        return plan

    def execute_deduplication(self, dry_run: bool = True) -> bool:
        """æ‰§è¡Œå»é‡æ“ä½œ"""
        if not self.duplicate_groups:
            print("âœ… æ²¡æœ‰é‡å¤æ–‡ä»¶éœ€è¦å¤„ç†")
            return True

        plan = self.create_deduplication_plan()

        print(f"ğŸ“‹ å»é‡è®¡åˆ’:")
        print(f"   ä¿ç•™æ–‡ä»¶: {len(plan['keep_files'])} ä¸ª")
        print(f"   åˆ é™¤æ–‡ä»¶: {len(plan['remove_files'])} ä¸ª")

        if dry_run:
            print("ğŸ” é¢„æ¼”æ¨¡å¼ - ä¸ä¼šå®é™…åˆ é™¤æ–‡ä»¶")
            for remove_file in plan['remove_files']:
                kept_as = plan['backup_info'][remove_file]['kept_as']
                print(f"   ğŸ—‘ï¸ å°†åˆ é™¤: {Path(remove_file).name}")
                print(f"     ä¿ç•™ä¸º: {Path(kept_as).name}")
            return True

        # å®é™…æ‰§è¡Œåˆ é™¤
        success_count = 0

        for remove_file_str in plan['remove_files']:
            try:
                remove_file = Path(remove_file_str)
                if remove_file.exists():
                    remove_file.unlink()
                    success_count += 1
                    print(f"ğŸ—‘ï¸ å·²åˆ é™¤: {remove_file.name}")

            except Exception as e:
                print(f"âŒ åˆ é™¤å¤±è´¥ {remove_file}: {e}")

        print(f"âœ… æˆåŠŸåˆ é™¤ {success_count} ä¸ªé‡å¤æ–‡ä»¶")

        # ä¿å­˜å»é‡ä¿¡æ¯
        dedup_log = self.vault_path / 'deduplication_log.json'
        with open(dedup_log, 'w', encoding='utf-8') as f:
            json.dump(plan, f, indent=2, ensure_ascii=False)

        print(f"ğŸ“„ å»é‡æ—¥å¿—å·²ä¿å­˜: {dedup_log}")

        return True

def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description='åˆ†æå’Œè§£å†³ç¬”è®°é‡å¤é—®é¢˜')
    parser.add_argument('vault_path', help='Obsidianåº“è·¯å¾„')
    parser.add_argument('--execute', action='store_true', help='å®é™…æ‰§è¡Œå»é‡ï¼ˆé»˜è®¤ä¸ºé¢„æ¼”ï¼‰')
    parser.add_argument('--report-only', action='store_true', help='ä»…ç”ŸæˆæŠ¥å‘Š')

    # å¦‚æœæ²¡æœ‰å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„è¿›è¡Œåˆ†æ
    if len(sys.argv) == 1:
        print("ğŸ” ä½¿ç”¨é»˜è®¤è·¯å¾„è¿›è¡Œé‡å¤åˆ†æ...")
        vault_paths = [
            '/tmp/debug_vault',
            '/tmp/test_vault_integration',
            '/tmp/obsidian_vault'
        ]

        for vault_path in vault_paths:
            if Path(vault_path).exists():
                print(f"\nåˆ†æè·¯å¾„: {vault_path}")
                analyzer = DuplicateNotesAnalyzer(vault_path)
                analyzer.find_duplicates()
                print(analyzer.generate_report())
        return

    args = parser.parse_args()

    analyzer = DuplicateNotesAnalyzer(args.vault_path)

    print(f"ğŸ” åˆ†æè·¯å¾„: {args.vault_path}")

    # æŸ¥æ‰¾é‡å¤
    analyzer.find_duplicates()

    # ç”ŸæˆæŠ¥å‘Š
    print(analyzer.generate_report())

    if args.report_only:
        return

    # æ‰§è¡Œå»é‡
    if analyzer.duplicate_groups:
        dry_run = not args.execute
        analyzer.execute_deduplication(dry_run=dry_run)

if __name__ == "__main__":
    main()